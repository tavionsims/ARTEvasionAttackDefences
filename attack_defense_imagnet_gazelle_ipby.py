# -*- coding: utf-8 -*-
"""attack_defense_imagnet_Gazelle.ipby

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t9vnQNEhc_amdO4sG0JWwcAaLYdBmbVW

# Basic workflow with ART for evasion attacks and defences

In this notebook we will show
- how to work with a Keras image classifier in ART
- how ART abstracts from the specific ML/DL backend
- how to apply a Projected Gradient Descent (PGD) evasion attack against that classifier
- how to deploy defences against such attacks
- how to create adversarial samples that can bypass those defences

Added by Ed Herranz (10/15/2022):
- how to protect against a white box attack agsint the defences

## Install and load prerequisites

You can preinstall all prerequisites by uncommenting and running the following cell.
"""

!pip install adversarial-robustness-toolbox

# Commented out IPython magic to ensure Python compatibility.


# Load basic dependencies:
import warnings
warnings.filterwarnings('ignore')

# %matplotlib inline
import matplotlib.pyplot as plt
import sys
import numpy as np

# Disable TensorFlow eager execution:
import tensorflow as tf
if tf.executing_eagerly():
    tf.compat.v1.disable_eager_execution()

# Load Keras dependencies:
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from tensorflow.keras.preprocessing import image

# Load ART dependencies:
from art.estimators.classification import KerasClassifier
from art.attacks.evasion import ProjectedGradientDescent
from art.defences.preprocessor import SpatialSmoothing
from art.utils import to_categorical

# Install ImageNet stubs:
!{sys.executable} -m pip install git+https://github.com/nottombrown/imagenet_stubs
import imagenet_stubs
from imagenet_stubs.imagenet_2012_labels import name_to_label, label_to_name

"""## Load images

We are going to load a set of 16 example images for illustration purposes.
"""

images_list = list()
for i, image_path in enumerate(imagenet_stubs.get_image_paths()):
    im = image.load_img(image_path, target_size=(224, 224))
    im = image.img_to_array(im)
    images_list.append(im)
    print(image_path)
    if 'gazelle.jpg' in image_path:
        # get gazelle index
        gazelle_idx = i
images = np.array(images_list)

"""The images all have a resolution of 224 x 224 pixels, and 3 color channels (RGB)."""

print('Number of images:', images.shape[0])
print('Dimension of images:', images.shape[1], 'x', images.shape[2], 'pixels')
print('Number of color channels:', images.shape[3], '(RGB)')

"""As default choice, we are going to use the unicycle image for illustration purposes. But you could use any other of the 16 images in the following (just change the value of the `idx` variable)."""

idx = gazelle_idx

plt.figure(figsize=(8,8)); plt.imshow(images[idx] / 255); plt.axis('off'); plt.show()

"""## Load ResNet50 classifier

Next we are going to use a state-of-the-art classifier on those images.
"""

# This loads the pretrained ResNet50 model:
model = ResNet50(weights='imagenet')

"""Let's look at the prediction that this model yields for the selected image:"""

# We need to expand the input dimension and apply the preprocessing required for ResNet50:
x = np.expand_dims(images[idx].copy(), axis=0)
x = preprocess_input(x)

# Then apply the model, determine the predicted label and confidence:
pred = model.predict(x)
label = np.argmax(pred, axis=1)[0]
confidence = pred[:,label][0]

print('Prediction:', label_to_name(label), '- confidence {0:.2f}'.format(confidence))

"""So the model correctly tells us that this image shows a unicycle/monocycle, which is good :-)

Next we will create an ART KerasClassifier wrapper around the model. <br>
We need to take care of the `preprocess_input` logic that has to be applied:

- swap the order of the color channels (RGB -> BGR)
- subtract the channel means
"""

from art.preprocessing.preprocessing import Preprocessor

class ResNet50Preprocessor(Preprocessor):

    def __call__(self, x, y=None):
        return preprocess_input(x.copy()), y

    def estimate_gradient(self, x, gradient):
        return gradient[..., ::-1]

# Create the ART preprocessor and classifier wrapper:
preprocessor = ResNet50Preprocessor()
classifier = KerasClassifier(clip_values=(0, 255), model=model, preprocessing=preprocessor)

"""Now we will apply the classifier object to obtain the prediction.

**Note:** we have to swap the color channel order (from RGB to BGR) before feeding the input to the classifier
"""

# Same as for the original model, we expand the dimension of the inputs.
x_art = np.expand_dims(images[idx], axis=0)

# Then apply the model through the classifier API, determine the predicted label and confidence:
pred = classifier.predict(x_art)
label = np.argmax(pred, axis=1)[0]
confidence = pred[:,label][0]

print('Prediction:', label_to_name(label), '- confidence {0:.2f}'.format(confidence))

"""So through the classifier API we obtain the same predictions as from the raw model, but now we have an abstraction from the actual backend (e.g. Keras).

The classifier wrapper allows us to call other functions besides predict.

For example, we can obtain the **loss gradient** of the classifier, which is used in many of the algorithms for adversarial sample generation:
"""

loss_gradient = classifier.loss_gradient(x=x_art, y=to_categorical([label], nb_classes=1000))

# Let's plot the loss gradient.
# First, swap color channels back to RGB order:
loss_gradient_plot = loss_gradient[0]

# Then normalize loss gradient values to be in [0,1]:
loss_gradient_min = np.min(loss_gradient)
loss_gradient_max = np.max(loss_gradient)
loss_gradient_plot = (loss_gradient_plot - loss_gradient_min)/(loss_gradient_max - loss_gradient_min)

# Show plot:
plt.figure(figsize=(8,8)); plt.imshow(loss_gradient_plot); plt.axis('off'); plt.show()

"""## Create adversarial samples

Next, we are going to create an adversarial sample. <br>
We are going to use **Projected Gradient Descent (PGD)**, which is one of the strongest existing attacks. <br>
We will first perform an **untargeted** adversarial attack.
"""

# Create the attacker:
adv = ProjectedGradientDescent(classifier, targeted=False, max_iter=10, eps_step=1, eps=5)

# Generate the adversarial sample:
x_art_adv = adv.generate(x_art)

# Plot the adversarial sample (note: we swap color channels back to RGB order):
plt.figure(figsize=(8,8)); plt.imshow(x_art_adv[0] / 255); plt.axis('off'); plt.show()

# And apply the classifier to it:
pred_adv = classifier.predict(x_art_adv)
label_adv = np.argmax(pred_adv, axis=1)[0]
confidence_adv = pred_adv[:, label_adv][0]
print('Prediction:', label_to_name(label_adv), '- confidence {0:.2f}'.format(confidence_adv))

"""Next, we will perform a **targeted attack** where we pick the class that we want the classifier to predict on the adversarial sample. <br>
Below is the list of labels and class names - make your pick!
"""

for i in range(1000):
    print('label', i, '-', label_to_name(i))

"""As default, let's get this image misclassified as black swan (label 100)!"""

target_label = 100

"""Now let's perform the targeted attack:"""

# Set the configuration to a targeted attack:
adv.set_params(targeted=True)

# Generate the adversarial sample:
x_art_adv = adv.generate(x_art, y=to_categorical([target_label]))

# Plot the adversarial sample (note: we swap color channels back to RGB order):
plt.figure(figsize=(8,8)); plt.imshow(x_art_adv[0] / 255); plt.axis('off'); plt.show()

# And apply the classifier to it:
pred_adv = classifier.predict(x_art_adv)
label_adv = np.argmax(pred_adv, axis=1)[0]
confidence_adv = pred_adv[:, label_adv][0]
print('Prediction:', label_to_name(label_adv), '- confidence {0:.2f}'.format(confidence_adv))

"""We can measure the quantity of perturbation that was added to the image using different $\ell_p$ norms. <br>
**Note:** the PGD attack controls the $\ell_\infty$ norm via the `epsilon` parameter.
"""

l_0 = int(99*len(np.where(np.abs(x_art[0] - x_art_adv[0])>0.5)[0]) / (224*224*3)) + 1
l_1 = int(99*np.sum(np.abs(x_art[0] - x_art_adv[0])) / np.sum(np.abs(x_art[0]))) + 1
l_2 = int(99*np.linalg.norm(x_art[0] - x_art_adv[0]) / np.linalg.norm(x_art[0])) + 1
l_inf = int(99*np.max(np.abs(x_art[0] - x_art_adv[0])) / 255) + 1

print('Perturbation l_0 norm: %d%%' % l_0)
print('Perturbation l_1 norm: %d%%' % l_1)
print('Perturbation l_2 norm: %d%%' % l_2)
print('Noise l_inf norm: %d%%' % l_inf)

# Let's also plot the absolute amount of adversarial pixel perturbations:
pert = np.abs(x_art[0] - x_art_adv[0])[..., ::-1]
pert_min = np.min(pert)
pert_max = np.max(pert)
plt.figure(figsize=(8,8)); plt.imshow((pert - pert_min) / (pert_max - pert_min)); plt.axis('off'); plt.show()

"""## Apply defences

Next we are going to apply a simple input preprocessing defence: Spatial Smoothing. <br>
Ideally, we want this defence to result in correct predictions when applied both to the original and the adversarial images.
"""

import pandas as pd

# Values from 3 to 10 for the window size
window_sizes = range(3, 11)

# Initialize the SpatialSmoothing defence.
ss = SpatialSmoothing(window_size=3)

# Table header
table_header = ["Window Size", "Original Pred. Class", "Original Pred. Confidence", "Adv. Pred. Class", "Adv. Pred. Confidence"]
table_data = []

# Loop through window sizes
for window_size in window_sizes:
    # Apply the defense to the original input and to the adversarial sample, respectively
    x_art_def, _ = ss(x_art)
    x_art_adv_def, _ = ss(x_art_adv)

    # Compute the classifier predictions on the preprocessed inputs
    pred_def = classifier.predict(x_art_def)
    label_def = np.argmax(pred_def, axis=1)[0]
    confidence_def = pred_def[:, label_def][0]

    pred_adv_def = classifier.predict(x_art_adv_def)
    label_adv_def = np.argmax(pred_adv_def, axis=1)[0]
    confidence_adv_def = pred_adv_def[:, label_adv_def][0]

    # Append results to the table_data list
    table_data.append([window_size, label_def, confidence_def, label_adv_def, confidence_adv_def])

# Create a DataFrame
df = pd.DataFrame(table_data, columns=table_header)

# Display the DataFrame
print(df)

# Show the preprocessed adversarial sample:
plt.figure(figsize=(8,8)); plt.imshow(x_art_adv_def[0] / 255); plt.axis('off'); plt.show()

"""# **Question 2 Response**
The code is supposed to be added to the applied Defenses section. The current spatial smoothing had a window size of 3. But the goal of this task was to rerun it with values 3 to 10 by using a loop. After the loop is completed, we should create a table for each window size for the prediction class and prediction confidence of the adversarial examples. So here we will run the code and examine the findings.

Since we added the code, we could now dive into our output to see if it ran like we wanted to and if we can analyze the prediction class and confidence. We applied the spatial smoothing defense with the window sizes that range from 3-10. Since we had to make it for the original and adv attack predictions, we should have 4 columns in our window output with the corresponding predictions. It appears the original prediction class had a very high confidence of 0.99% and once you ass spatial smoothing you can see the predictions reduce to 90% indicating that once the perturbations were added for the adversarial attacks, it lowered confidence.

# **Apply Defenses 2**
"""

from art.defences.preprocessor import FeatureSqueezing
import pandas as pd


bit_depths = [1, 2, 3, 4]


table_header_fs = ["Bit Depth", "Original Pred. Class", "Original Pred. Confidence", "Adv. Pred. Class", "Adv. Pred. Confidence"]
table_data_fs = []

for bit_depth in bit_depths:
    fs = FeatureSqueezing(bit_depth=bit_depth, clip_values=(0, 255))

    # Apply the defense to the original input and to the adversarial sample, respectively
    x_art_def_fs, _ = fs(x_art)
    x_art_adv_def_fs, _ = fs(x_art_adv)

    # Compute the classifier predictions on the preprocessed inputs
    pred_def_fs = classifier.predict(x_art_def_fs)
    label_def_fs = np.argmax(pred_def_fs, axis=1)[0]
    confidence_def_fs = pred_def_fs[:, label_def_fs][0]

    pred_adv_def_fs = classifier.predict(x_art_adv_def_fs)
    label_adv_def_fs = np.argmax(pred_adv_def_fs, axis=1)[0]
    confidence_adv_def_fs = pred_adv_def_fs[:, label_adv_def_fs][0]

    # Append results to the table_data_fs list
    table_data_fs.append([bit_depth, label_def_fs, confidence_def_fs, label_adv_def_fs, confidence_adv_def_fs])

# Create a DataFrame for FeatureSqueezing
df_fs = pd.DataFrame(table_data_fs, columns=table_header_fs)

# Display the DataFrame for FeatureSqueezing
print(df_fs)

"""# **Question 4**

Honestly, I am very surprised at the results, and it really shows you the impact that feature squeezing has on your model. You can see from the output that the depth increases. You can see that the pred class 556 had a 55% prediction and from 353 down it was around a 98-99% original prediction rate. But after the adversarial prediction, the model predicts the 556 class at a 71% confidence level which means that the attack and the perturbation has intruded the model therefore leading to compromised predictions.

Even with the 353 class you can see as the depth increases that the original predictions had a very high prediction rate around 98-99% which is very good. That is exactly what you want your model to predict at that rate and try to get as close to 100% as you can. But when we added the adversarial attacks and predictions you can see that the confidence rate fluctuated drastically. It went from 98-99% accuracy to a predicting 52-69% for the 353 class after the adversarial attacks. You can see that feature squeezing really introduces uncertainty in the model. Even though it’s not fun to see your model’s accuracy depleted like that, it helps you go on to build the model more robust.

## Perform adaptive whitebox attack to defeat defences

Next we are going to mount an adaptive whitebox attack in which the attacker aims at defeating the defence that we just put into place.

First, we create a classifier which incorporates the defence:
"""

classifier_def = KerasClassifier(preprocessing=preprocessor, preprocessing_defences=[ss], clip_values=(0, 255),
                                 model=model)

# Now we apply this classifier to the adversarial sample from before:
pred_def = classifier_def.predict(x_art_adv)
label_def = np.argmax(pred_def, axis=1)[0]
confidence_def = pred_def[:, label_def][0]

print('Prediction:', label_to_name(label_def), '- confidence {0:.2f}'.format(confidence_def))

"""We observe that this classifier reproduces the prediction that we had obtained before by manually applying the input preprocessing defence.

Now we create an adversarial sample against the *defended* classifier. <br>
As we are going to see, this adversarial sample is able to bypass the input preprocessing defence.
"""

# Create the attacker.
# Note: here we use a larger number of iterations to achieve the same level of confidence in the misclassification
adv_def = ProjectedGradientDescent(classifier_def, targeted=True, max_iter=40, eps_step=1, eps=5)

# Generate the adversarial sample:
x_art_adv_def = adv_def.generate(x_art, y=to_categorical([target_label]))

# Plot the adversarial sample (note: we swap color channels back to RGB order):
plt.figure(figsize=(8,8)); plt.imshow(x_art_adv_def[0] / 255); plt.axis('off'); plt.show()

# And apply the classifier to it:
pred_adv = classifier_def.predict(x_art_adv_def)
label_adv = np.argmax(pred_adv, axis=1)[0]
confidence_adv = pred_adv[:, label_adv][0]
print('Prediction:', label_to_name(label_adv), '- confidence {0:.2f}'.format(confidence_adv))

"""Let's also look at the $\ell_p$ norms of that adversarial perturbation:"""

l_0 = int(99*len(np.where(np.abs(x_art[0] - x_art_adv_def[0])>0.5)[0]) / (224*224*3)) + 1
l_1 = int(99*np.sum(np.abs(x_art[0] - x_art_adv_def[0])) / np.sum(np.abs(x_art[0]))) + 1
l_2 = int(99*np.linalg.norm(x_art[0] - x_art_adv_def[0]) / np.linalg.norm(x_art[0])) + 1
l_inf = int(99*np.max(np.abs(x_art[0] - x_art_adv_def[0])) / 255) + 1

print('Perturbation l_0 norm: %d%%' % l_0)
print('Perturbation l_1 norm: %d%%' % l_1)
print('Perturbation l_2 norm: %d%%' % l_2)
print('Noise l_inf norm: %d%%' % l_inf)

from art.defences.preprocessor import JpegCompression
from art.defences.postprocessor import GaussianNoise
import pandas as pd


# Values for Jpeg compression (from 20 to 90 in increments of 5)
compression_levels = range(20, 91, 5)

# Values for GaussianNoise scale (from 0.05 to 1.5 in increments of 0.10)
noise_scales = [round(scale, 2) for scale in np.arange(0.05, 1.6, 0.1)]

# Initialize the JpegCompression defense.
jpeg_def = JpegCompression(quality=20, clip_values=(0, 255))  # Specify clip_values

# Table header
table_header = ["Compression Level", "Noise Scale", "Original Pred. Class", "Original Pred. Confidence", "Adv. Pred. Class", "Adv. Pred. Confidence"]
table_data = []

# Loop through compression levels and noise scales
for compression_level in compression_levels:
    for noise_scale in noise_scales:
        # Apply the JpegCompression defense to the original input and to the adversarial sample, respectively
        x_art_def = jpeg_def(x_art)
        x_art_adv_def = jpeg_def(x_art_adv)

        # Apply the GaussianNoise defense to the pre-processed inputs
        noise_def = GaussianNoise(scale=noise_scale)
        x_art_def = noise_def(x_art_def[0])  # Extract the NumPy array from the tuple
        x_art_adv_def = noise_def(x_art_adv_def[0])  # Extract the NumPy array from the tuple

        # Clip the values to the specified range
        x_art_def = np.clip(x_art_def, 0, 255)
        x_art_adv_def = np.clip(x_art_adv_def, 0, 255)

        # Compute the classifier predictions on the preprocessed inputs
        pred_def = classifier.predict(x_art_def)
        label_def = np.argmax(pred_def, axis=1)[0]
        confidence_def = pred_def[:, label_def][0]

        pred_adv_def = classifier.predict(x_art_adv_def)
        label_adv_def = np.argmax(pred_adv_def, axis=1)[0]
        confidence_adv_def = pred_adv_def[:, label_adv_def][0]

        # Append results to the table_data list
        table_data.append([compression_level, noise_scale, label_def, confidence_def, label_adv_def, confidence_adv_def])

# Create a DataFrame
df = pd.DataFrame(table_data, columns=table_header)

# Display the DataFrame
print(df)

# Show the preprocessed adversarial sample:
plt.figure(figsize=(8, 8))
plt.imshow(x_art_adv_def[0] / 255)
plt.axis('off')
plt.show()

"""# Question 3
The code is supposed to summarize the Jpeg and Gaussian Noise scales which had to be applied to the adversarial attacks and the original examples. This code took me the longest, ran into a lot of errors and a bunch of code rewriting, most of it was because of the JPEG function and how it was intaking the image. I had to reshape it in a way that would run the image.
Since we added the code successfully, we could now dive into our output to see if it ran like we wanted to and if we can analyze the prediction class and confidence. First off, the range for the compression levels was 20-90 and the prediction class stays the same. Honestly this is high prediction scores, and I wasn’t expecting this. It actually indicates that the defenses did not alter the model and that it stayed resilient through the perturbations which is a good sign!

Comparing with the previous adversarial sample, the $\ell_0$ and $\ell_1$ norms have slightly increased, while $\ell_2$ and $\ell_\infty$ norms have stayed the same (the latter not being surprising as the PGD attack controls the $\ell_\infty$ norm budget).

## Conclusions

We have walked through an end-to-end example of using a Keras image classifier in ART, creating adversarial samples, deploying input preprocessing defences and, finally, bypassing those defences in an adaptive white-box attack.
"""